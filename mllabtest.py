# -*- coding: utf-8 -*-
"""MLLabTest.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ICDX_AEDPZscrknyPd9TR6Ul_8MkuHLY
"""

# QUESTION NO:1

# Import necessary libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_curve, roc_auc_score
from sklearn.linear_model import LogisticRegression,LinearRegression,Perceptron
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.cluster import KMeans
from sklearn.neural_network import MLPClassifier
from sklearn.naive_bayes import GaussianNB
import seaborn as sns

# Step 1: Loading the dataset
data = pd.read_csv("/content/drive/MyDrive/parkinsons.data")

data.columns

data.head(10)

data.shape

# Step2: Pre-processing the data

# Finding missing values
print("Missing values:\n", data.isna().sum())

"""
No NULL Values observed, hence no need of handling NULL values`

"""

# Step 4: Feature Extraction
# Encoding of the 'name' column is not necessary as its correlation is minimal and can be ignored
data.drop(columns=['name'], inplace=True)

# Step2: Pre-processing the data
# Standarization

from sklearn.preprocessing import StandardScaler
object= StandardScaler()
scale = object.fit_transform(data)
print(scale)

# Step3: Exploratory Data Analysis

sns.barplot(data, x="MDVP:Fo(Hz)", hue="status")

print(data.describe())

# We try to remove the columns with low correlation to the target column or select the features with high correlation
import matplotlib.pyplot as plt
corr_matrix = data.corr()
plt.subplots(figsize=(20,15))
sns.heatmap(corr_matrix, annot=True)
plt.show()


# Since the correlation of every other column against status is high, we do not drop any other columns except name.

# Step 5: Split the data into training, testing, and validation sets
X = data.drop(columns=['status'])
y = data['status']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Step 6: Train the model
models = {
  # 'Linear Regression': LinearRegression(),
    'Logistic Regression': LogisticRegression(),
    'KNN': KNeighborsClassifier(),
    'SVM': SVC(),
    'MLP': MLPClassifier(),
    'Naive Bayes':GaussianNB(),
    'Percepton': Perceptron()
}
for name, model in models.items():
    model.fit(X_train, y_train)

# Step 7: Test the model
from sklearn import metrics
from sklearn.metrics import classification_report
results = {}
for name, model in models.items():
    y_pred = model.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
    confusion_matrix = metrics.confusion_matrix(y_test, y_pred)
    cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix, display_labels = [False, True])
    cm_display.plot()
    plt.show()
    print(classification_report(y_test, y_pred))

    results[name] = accuracy
    print(f"{name}: {accuracy}")

# Step 8: Measure the performance of the model
best_model = max(results, key=results.get)
print(f"Best model: {best_model} with accuracy: {results[best_model]}")

#INTERPRETATION: Logistic regression is often a good choice for binary classification problems,
#especially when the relationship between the features and the target variable is relatively linear.
#It's also interpretable and less prone to overfitting compared to more complex models like neural networks.

# Step 9: Represent the training and testing results using ROC curves
y_pred_proba = models[best_model].predict_proba(X_test)[:,1]
fpr1, tpr1, thresh1 = roc_curve(y_test, y_pred_proba, pos_label=1)
random_probs = [0 for i in range(len(y_test))]
p_fpr, p_tpr, _ = roc_curve(y_test, random_probs, pos_label=1)
plt.plot(fpr1, tpr1, linestyle='--',color='orange', label='Logistic Regression')
plt.plot(p_fpr, p_tpr, linestyle='--', color='blue')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve')
plt.show()

train_accuracy = accuracy_score(y_train, models[best_model].predict(X_train))
print(f"Training accuracy: {train_accuracy}")

# Step 10: KMeans Clustering
kmeans = KMeans(n_clusters=2) #No of clusters is 2 because the value of 'status' is boolean(1 or 0)
kmeans.fit(X)
labels = kmeans.labels_
print("Cluster performance:", accuracy_score(y, labels))
